{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43264513-825f-4d09-b667-70b9e7e32855",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n+---------+--------------------+-------------------+---------+-------------------+\n|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n+---------+--------------------+-------------------+---------+-------------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+\n|user_id|user_first_name|user_last_name|          user_email|\n+-------+---------------+--------------+--------------------+\n|      1|         Sandra|        Karpov|    skarpov0@ovh.net|\n|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n|      5|         Loreen|         Malin|lmalin4@independe...|\n|      6|           Augy|      Christon|  achriston5@mlb.com|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|\n|      9|        Vassily|         Tamas|vtamas8@businessw...|\n|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n+-------+---------------+--------------+--------------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+----------+\n|course_enrolment_id|user_id|course_id|price_paid|\n+-------------------+-------+---------+----------+\n|                  1|     10|        2|      9.99|\n|                  2|      5|        2|      9.99|\n|                  3|      7|        5|     10.99|\n|                  4|      9|        2|      9.99|\n|                  5|      8|        2|      9.99|\n|                  6|      5|        5|     10.99|\n|                  7|      4|        5|     10.99|\n|                  8|      7|        3|     10.99|\n|                  9|      8|        5|     10.99|\n|                 10|      3|        3|     10.99|\n|                 11|      7|        5|     10.99|\n|                 12|      3|        2|      9.99|\n|                 13|      5|        2|      9.99|\n|                 14|      4|        3|     10.99|\n|                 15|      8|        2|      9.99|\n+-------------------+-------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "%run \"./33_setup_data_sets_to_perform_joins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccdaad2b-9732-454c-b496-8686d39508d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n\njoin(other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Joins with another :class:`DataFrame`, using the given join expression.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    other : :class:`DataFrame`\n        Right side of the join\n    on : str, list or :class:`Column`, optional\n        a string for the join column name, a list of column names,\n        a join expression (Column), or a list of Columns.\n        If `on` is a string or a list of strings indicating the name of the join column(s),\n        the column(s) must exist on both sides, and this performs an equi-join.\n    how : str, optional\n        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n        ``anti``, ``leftanti`` and ``left_anti``.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        Joined DataFrame.\n    \n    Examples\n    --------\n    The following examples demonstrate various join types among ``df1``, ``df2``, and ``df3``.\n    \n    >>> import pyspark.sql.functions as sf\n    >>> from pyspark.sql import Row\n    >>> df = spark.createDataFrame([Row(name=\"Alice\", age=2), Row(name=\"Bob\", age=5)])\n    >>> df2 = spark.createDataFrame([Row(name=\"Tom\", height=80), Row(name=\"Bob\", height=85)])\n    >>> df3 = spark.createDataFrame([\n    ...     Row(name=\"Alice\", age=10, height=80),\n    ...     Row(name=\"Bob\", age=5, height=None),\n    ...     Row(name=\"Tom\", age=None, height=None),\n    ...     Row(name=None, age=None, height=None),\n    ... ])\n    \n    Inner join on columns (default)\n    \n    >>> df.join(df2, \"name\").show()\n    +----+---+------+\n    |name|age|height|\n    +----+---+------+\n    | Bob|  5|    85|\n    +----+---+------+\n    \n    >>> df.join(df3, [\"name\", \"age\"]).show()\n    +----+---+------+\n    |name|age|height|\n    +----+---+------+\n    | Bob|  5|  NULL|\n    +----+---+------+\n    \n    Outer join on a single column with an explicit join condition.\n    \n    When the join condition is explicited stated: `df.name == df2.name`, this will\n    produce all records where the names match, as well as those that don't (since\n    it's an outer join). If there are names in `df2` that are not present in `df`,\n    they will appear with `NULL` in the `name` column of `df`, and vice versa for `df2`.\n    \n    >>> joined = df.join(df2, df.name == df2.name, \"outer\").sort(sf.desc(df.name))\n    >>> joined.show() # doctest: +SKIP\n    +-----+----+----+------+\n    | name| age|name|height|\n    +-----+----+----+------+\n    |  Bob|   5| Bob|    85|\n    |Alice|   2|NULL|  NULL|\n    | NULL|NULL| Tom|    80|\n    +-----+----+----+------+\n    \n    To unambiguously select output columns, specify the dataframe along with the column name:\n    \n    >>> joined.select(df.name, df2.height).show() # doctest: +SKIP\n    +-----+------+\n    | name|height|\n    +-----+------+\n    |  Bob|    85|\n    |Alice|  NULL|\n    | NULL|    80|\n    +-----+------+\n    \n    However, in self-joins, direct column references can cause ambiguity:\n    \n    >>> df.join(df, df.name == df.name, \"outer\").select(df.name).show() # doctest: +SKIP\n    Traceback (most recent call last):\n    ...\n    pyspark.errors.exceptions.captured.AnalysisException: Column name#0 are ambiguous...\n    \n    A better approach is to assign aliases to the dataframes, and then reference\n    the ouptut columns from the join operation using these aliases:\n    \n    >>> df.alias(\"a\").join(\n    ...     df.alias(\"b\"), sf.col(\"a.name\") == sf.col(\"b.name\"), \"outer\"\n    ... ).sort(sf.desc(\"a.name\")).select(\"a.name\", \"b.age\").show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |  Bob|  5|\n    |Alice|  2|\n    +-----+---+\n    \n    Outer join on a single column with implicit join condition using column name\n    \n    When you provide the column name directly as the join condition, Spark will treat\n    both name columns as one, and will not produce separate columns for `df.name` and\n    `df2.name`. This avoids having duplicate columns in the output.\n    \n    >>> df.join(df2, \"name\", \"outer\").sort(sf.desc(\"name\")).show()\n    +-----+----+------+\n    | name| age|height|\n    +-----+----+------+\n    |  Tom|NULL|    80|\n    |  Bob|   5|    85|\n    |Alice|   2|  NULL|\n    +-----+----+------+\n    \n    Outer join on multiple columns\n    \n    >>> df.join(df3, [\"name\", \"age\"], \"outer\").show()\n    +-----+----+------+\n    | name| age|height|\n    +-----+----+------+\n    | NULL|NULL|  NULL|\n    |Alice|   2|  NULL|\n    |Alice|  10|    80|\n    |  Bob|   5|  NULL|\n    |  Tom|NULL|  NULL|\n    +-----+----+------+\n    \n    Left outer join on columns\n    \n    >>> df.join(df2, \"name\", \"left_outer\").show()\n    +-----+---+------+\n    | name|age|height|\n    +-----+---+------+\n    |Alice|  2|  NULL|\n    |  Bob|  5|    85|\n    +-----+---+------+\n    \n    Right outer join on columns\n    \n    >>> df.join(df2, \"name\", \"right_outer\").show()\n    +----+----+------+\n    |name| age|height|\n    +----+----+------+\n    | Tom|NULL|    80|\n    | Bob|   5|    85|\n    +----+----+------+\n    \n    Left semi join on columns\n    \n    >>> df.join(df2, \"name\", \"left_semi\").show()\n    +----+---+\n    |name|age|\n    +----+---+\n    | Bob|  5|\n    +----+---+\n    \n    Left anti join on columns\n    \n    >>> df.join(df2, \"name\", \"left_anti\").show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  2|\n    +-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "help(courses_df.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355419d1-21f4-473d-8e98-ec5b5be76c4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|course_id|price_paid|\n+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|        3|     10.99|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|        2|      9.99|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                  7|        5|     10.99|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                 14|        3|     10.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                  2|        2|      9.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                  6|        5|     10.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                 13|        2|      9.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  3|        5|     10.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  8|        3|     10.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                 11|        5|     10.99|\n|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|                  5|        2|      9.99|\n|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|                  9|        5|     10.99|\n|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|                 15|        2|      9.99|\n|      9|        Vassily|         Tamas|vtamas8@businessw...|                  4|        2|      9.99|\n|     10|          Wells|      Simpkins|wsimpkins9@amazon...|                  1|        2|      9.99|\n+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "users_df.join(course_enrolments_df, on='user_id', how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f79b41-52b4-4127-8b5f-3ed00031f568",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|user_id|user_first_name|user_last_name|          user_email|course_enrolment_id|user_id|course_id|price_paid|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|      3|        3|     10.99|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|      3|        2|      9.99|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                  7|      4|        5|     10.99|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                 14|      4|        3|     10.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                  2|      5|        2|      9.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                  6|      5|        5|     10.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                 13|      5|        2|      9.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  3|      7|        5|     10.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  8|      7|        3|     10.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                 11|      7|        5|     10.99|\n|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|                  5|      8|        2|      9.99|\n|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|                  9|      8|        5|     10.99|\n|      8|         Nadine|     Grimsdell|ngrimsdell7@sohu.com|                 15|      8|        2|      9.99|\n|      9|        Vassily|         Tamas|vtamas8@businessw...|                  4|      9|        2|      9.99|\n|     10|          Wells|      Simpkins|wsimpkins9@amazon...|                  1|     10|        2|      9.99|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# the column on which the dataframes are joined get repeated in the result with this syntax\n",
    "\n",
    "users_df.alias(\"u\").join(\n",
    "    course_enrolments_df.alias(\"ce\"), users_df[\"user_id\"] == course_enrolments_df[\"user_id\"], how=\"inner\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c1e8861-5303-46a1-a571-78326b52404f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Join example\n",
    "\n",
    "* Find the total number of courses enrolled by each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d32844-7908-4fe7-88a0-e3e8537aaebd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n|user_id|courses_enrolled|\n+-------+----------------+\n|      5|               3|\n|      7|               3|\n|      8|               3|\n|      3|               2|\n|      4|               2|\n|      9|               1|\n|     10|               1|\n|      1|               0|\n|      2|               0|\n|      6|               0|\n+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "result_df = (\n",
    "    users_df.alias(\"u\")\n",
    "    .join(course_enrolments_df.alias(\"ce\"), on=[\"user_id\"], how=\"left\")\n",
    "    .groupBy(\"u.user_id\")\n",
    "    .agg(\n",
    "        F.sum(F.when(F.col(\"course_enrolment_id\").isNull(), 0).otherwise(1)).alias(\n",
    "            \"courses_enrolled\"\n",
    "        )\n",
    "    )\n",
    "    .orderBy(F.desc(\"courses_enrolled\"))\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c03cca29-55e1-41ca-b92e-9c7731237618",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Another way of writing the same query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040d1a94-a098-434c-995e-344041a4cdcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n|user_id|courses_enrolled|\n+-------+----------------+\n|      5|               3|\n|      7|               3|\n|      8|               3|\n|      3|               2|\n|      4|               2|\n|      9|               1|\n|     10|               1|\n|      1|               0|\n|      2|               0|\n|      6|               0|\n+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "result_df = (\n",
    "    users_df.alias(\"u\")\n",
    "    .join(course_enrolments_df.alias(\"ce\"), on=[\"user_id\"], how=\"left\")\n",
    "    .groupBy(\"u.user_id\")\n",
    "    .agg(\n",
    "        F.sum(\n",
    "            F.expr(\n",
    "                \"\"\"\n",
    "                    CASE \n",
    "                        WHEN \n",
    "                            ce.course_enrolment_id is NULL \n",
    "                        THEN 0 \n",
    "                        ELSE 1 \n",
    "                    END\n",
    "                \"\"\"\n",
    "            )\n",
    "        ).alias(\"courses_enrolled\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"courses_enrolled\"))\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "34_joins",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
