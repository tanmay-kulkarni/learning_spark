{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d24c4d21-9426-4db8-88ea-d22c40be4f8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Type of variable `spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea5622f-f5ca-45b1-8482-aa75c26aa43a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a4ec236-02ea-4ea3-a672-9129640c7f59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n\ncreateDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n    or a :class:`numpy.ndarray`.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    data : :class:`RDD` or iterable\n        an RDD of any kind of SQL data representation (:class:`Row`,\n        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n        column names, default is None. The data type string format equals to\n        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n        omit the ``struct<>``.\n    \n        When ``schema`` is a list of column names, the type of each column\n        will be inferred from ``data``.\n    \n        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n        from ``data``, which should be an RDD of either :class:`Row`,\n        :class:`namedtuple`, or :class:`dict`.\n    \n        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n        match the real data, or an exception will be thrown at runtime. If the given schema is\n        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n        later.\n    samplingRatio : float, optional\n        the sample ratio of rows used for inferring. The first few rows will be used\n        if ``samplingRatio`` is ``None``.\n    verifySchema : bool, optional\n        verify data types of every row against schema. Enabled by default.\n    \n        .. versionadded:: 2.1.0\n    \n    Returns\n    -------\n    :class:`DataFrame`\n    \n    Notes\n    -----\n    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n    \n    Examples\n    --------\n    Create a DataFrame from a list of tuples.\n    \n    >>> spark.createDataFrame([('Alice', 1)]).show()\n    +-----+---+\n    |   _1| _2|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    \n    Create a DataFrame from a list of dictionaries.\n    \n    >>> d = [{'name': 'Alice', 'age': 1}]\n    >>> spark.createDataFrame(d).show()\n    +---+-----+\n    |age| name|\n    +---+-----+\n    |  1|Alice|\n    +---+-----+\n    \n    Create a DataFrame with column names specified.\n    \n    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    \n    Create a DataFrame with the explicit schema specified.\n    \n    >>> from pyspark.sql.types import *\n    >>> schema = StructType([\n    ...    StructField(\"name\", StringType(), True),\n    ...    StructField(\"age\", IntegerType(), True)])\n    >>> spark.createDataFrame([('Alice', 1)], schema).show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    \n    Create a DataFrame with the schema in DDL formatted string.\n    \n    >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    \n    Create an empty DataFrame.\n    When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n    as the DataFrame lacks data from which the schema can be inferred.\n    \n    >>> spark.createDataFrame([], \"name: string, age: int\").show()\n    +----+---+\n    |name|age|\n    +----+---+\n    +----+---+\n    \n    Create a DataFrame from Row objects.\n    \n    >>> from pyspark.sql import Row\n    >>> Person = Row('name', 'age')\n    >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n    >>> df.show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    \n    Create a DataFrame from a pandas DataFrame.\n    \n    >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n    +---+---+\n    |  0|  1|\n    +---+---+\n    |  1|  2|\n    +---+---+\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b0491e0-ef4a-43bf-ad1f-b3daaa10c3f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([1,2,3], 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19e5b28-12d4-41b4-b757-b655f709f435",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|value|\n+-----+\n|    1|\n|    2|\n|    3|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6266046e-7951-45d3-b149-391fcb1ebf17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54bf912e-4cad-4159-9a54-9608ad1164e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|value|\n+-----+\n|    1|\n|    2|\n|    3|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "df = spark.createDataFrame([1,2,3], StringType())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2185327-626b-4df5-9a67-569d4addfc35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating a dataframe using a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8d7daa-2b87-4a8e-8805-5c32b275c7ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n|user_id|user_name|\n+-------+---------+\n|      1|   Tanmay|\n|      2|    Roger|\n|      3|    David|\n|      4|    Emily|\n+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "users = [(1, 'Tanmay'), (2, 'Roger'), (3, 'David'), (4, 'Emily')]\n",
    "df = spark.createDataFrame(users, 'user_id int, user_name string')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a119dd26-6849-438c-bc46-50b15fb20861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([Row(user_id=1, user_name='Tanmay'),\n",
       "  Row(user_id=2, user_name='Roger'),\n",
       "  Row(user_id=3, user_name='David'),\n",
       "  Row(user_id=4, user_name='Emily')],\n",
       " pyspark.sql.types.Row)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect(), type(df.collect()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a13915d-f66f-4603-98f6-7e35c2241fed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b708ca-2790-4082-bf48-87d459931ab0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Row in module pyspark.sql.types:\n\nclass Row(builtins.tuple)\n |  Row(*args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n |  \n |  A row in :class:`DataFrame`.\n |  The fields in it can be accessed:\n |  \n |  * like attributes (``row.key``)\n |  * like dictionary values (``row[key]``)\n |  \n |  ``key in row`` will search through row keys.\n |  \n |  Row can be used to create a row object by using named arguments.\n |  It is not allowed to omit a named argument to represent that the value is\n |  None or missing. This should be explicitly set to None in this case.\n |  \n |  .. versionchanged:: 3.0.0\n |      Rows created from named arguments no longer have\n |      field names sorted alphabetically and will be ordered in the position as\n |      entered.\n |  \n |  Examples\n |  --------\n |  >>> from pyspark.sql import Row\n |  >>> row = Row(name=\"Alice\", age=11)\n |  >>> row\n |  Row(name='Alice', age=11)\n |  >>> row['name'], row['age']\n |  ('Alice', 11)\n |  >>> row.name, row.age\n |  ('Alice', 11)\n |  >>> 'name' in row\n |  True\n |  >>> 'wrong_key' in row\n |  False\n |  \n |  Row also can be used to create another Row like class, then it\n |  could be used to create Row objects, such as\n |  \n |  >>> Person = Row(\"name\", \"age\")\n |  >>> Person\n |  <Row('name', 'age')>\n |  >>> 'name' in Person\n |  True\n |  >>> 'wrong_key' in Person\n |  False\n |  >>> Person(\"Alice\", 11)\n |  Row(name='Alice', age=11)\n |  \n |  This form can also be used to create rows as tuple values, i.e. with unnamed\n |  fields.\n |  \n |  >>> row1 = Row(\"Alice\", 11)\n |  >>> row2 = Row(name=\"Alice\", age=11)\n |  >>> row1 == row2\n |  True\n |  \n |  Method resolution order:\n |      Row\n |      builtins.tuple\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __call__(self, *args: Any) -> 'Row'\n |      create new Row object\n |  \n |  __contains__(self, item: Any) -> bool\n |      Return key in self.\n |  \n |  __getattr__(self, item: str) -> Any\n |  \n |  __getitem__(self, item: Any) -> Any\n |      Return self[key].\n |  \n |  __reduce__(self) -> Union[str, Tuple[Any, ...]]\n |      Returns a tuple so Python knows how to pickle Row.\n |  \n |  __repr__(self) -> str\n |      Printable representation of Row used in Python REPL.\n |  \n |  __setattr__(self, key: Any, value: Any) -> None\n |      Implement setattr(self, name, value).\n |  \n |  asDict(self, recursive: bool = False) -> Dict[str, Any]\n |      Return as a dict\n |      \n |      Parameters\n |      ----------\n |      recursive : bool, optional\n |          turns the nested Rows to dict (default: False).\n |      \n |      Notes\n |      -----\n |      If a row contains duplicate field names, e.g., the rows of a join\n |      between two :class:`DataFrame` that both have the fields of same names,\n |      one of the duplicate fields will be selected by ``asDict``. ``__getitem__``\n |      will also return one of the duplicate fields, however returned value might\n |      be different to ``asDict``.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n |      True\n |      >>> row = Row(key=1, value=Row(name='a', age=2))\n |      >>> row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n |      True\n |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n |      True\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(cls, *args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from builtins.tuple:\n |  \n |  __add__(self, value, /)\n |      Return self+value.\n |  \n |  __eq__(self, value, /)\n |      Return self==value.\n |  \n |  __ge__(self, value, /)\n |      Return self>=value.\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __getnewargs__(self, /)\n |  \n |  __gt__(self, value, /)\n |      Return self>value.\n |  \n |  __hash__(self, /)\n |      Return hash(self).\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __le__(self, value, /)\n |      Return self<=value.\n |  \n |  __len__(self, /)\n |      Return len(self).\n |  \n |  __lt__(self, value, /)\n |      Return self<value.\n |  \n |  __mul__(self, value, /)\n |      Return self*value.\n |  \n |  __ne__(self, value, /)\n |      Return self!=value.\n |  \n |  __rmul__(self, value, /)\n |      Return value*self.\n |  \n |  count(self, value, /)\n |      Return number of occurrences of value.\n |  \n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from builtins.tuple:\n |  \n |  __class_getitem__(...) from builtins.type\n |      See PEP 585\n\n"
     ]
    }
   ],
   "source": [
    "help(Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b02b10f5-6d6f-46d8-a467-b82dd79861d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "row1 = Row(name='Tanmay', age=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2085eb92-8c62-4341-912b-474b3dbce04b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('Tanmay', 'Tanmay')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1['name'], row1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a650c71-cc7c-435f-84b5-1ceb823f62a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating a dataframe using a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a55c81-4009-4e0b-a8b1-bd23998a7343",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n|user_id|user_name|\n+-------+---------+\n|      1|   Tanmay|\n|      2|    Roger|\n|      3|    Scott|\n|      4|     Phil|\n|      5|    Sarah|\n+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "users = [[1, 'Tanmay'], [2, 'Roger'], [3, 'Scott'], [4, 'Phil'], [5, 'Sarah']]\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "users_as_Row = [Row(*user) for user in users]\n",
    "\n",
    "df = spark.createDataFrame(users_as_Row, 'user_id int, user_name string')\n",
    "\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_creating_a_dataframe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
