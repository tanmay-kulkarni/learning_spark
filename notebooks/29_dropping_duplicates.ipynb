{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e12c03e-202a-48c7-b84e-5e8350198795",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n| id|name|age|\n+---+----+---+\n|  1|John| 25|\n|  2|Jane| 30|\n|  3|John| 25|\n|  4|Mike| 35|\n|  5|Jane| 30|\n+---+----+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create a list of data with some duplicate records and some not\n",
    "data = [\n",
    "    (1, \"John\", 25),\n",
    "    (2, \"Jane\", 30),\n",
    "    (3, \"John\", 25),\n",
    "    (4, \"Mike\", 35),\n",
    "    (5, \"Jane\", 30)\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9617f61-a327-479b-ae2b-3f3f3a66d9b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### distinct will return unique rows. It doesn't take any arguements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c27de6f-7760-4926-90db-1da5646ff3b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n|name|age|\n+----+---+\n|John| 25|\n|Jane| 30|\n|Mike| 35|\n+----+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.select('name', 'age').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a0e6381-24f5-4a5d-b8aa-f3921e5cc1e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### drop_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5cc6989-528a-4a88-97f8-0e9fbded0783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n\ndropDuplicates(subset=None) method of pyspark.sql.dataframe.DataFrame instance\n    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n    \n    .. versionadded:: 1.4\n\n"
     ]
    }
   ],
   "source": [
    "help(df.drop_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb2f6d7c-29d0-46b6-b33f-37f3599e9fd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### by default, it will consider all columns as subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8440b4ee-f1ef-4955-8623-50b348b55e67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n| id|name|age|\n+---+----+---+\n|  1|John| 25|\n|  2|Jane| 30|\n|  3|John| 25|\n|  4|Mike| 35|\n|  5|Jane| 30|\n+---+----+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "322872ef-e827-4819-a357-d33aac45eb9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### pass subset to specify the columns over which duplicates are to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c674f3d-aad8-419d-ad5c-63af9f62e526",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n| id|name|age|\n+---+----+---+\n|  1|John| 25|\n|  2|Jane| 30|\n|  4|Mike| 35|\n+---+----+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['name', 'age']).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "29_dropping_duplicates",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
