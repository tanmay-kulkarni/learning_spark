{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a755a998-f71e-49e0-b733-61ecbd876794",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./07_create_a_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121710a3-782c-40c6-b603-f03c18bf971c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, concat, col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac731ba9-ac04-480c-8e24-3ed7f01110f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function split in module pyspark.sql.functions.builtin:\n\nsplit(str: 'ColumnOrName', pattern: str, limit: int = -1) -> pyspark.sql.column.Column\n    Splits str around matches of the given pattern.\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    str : :class:`~pyspark.sql.Column` or str\n        a string expression to split\n    pattern : str\n        a string representing a regular expression. The regex string should be\n        a Java regular expression.\n    limit : int, optional\n        an integer which controls the number of times `pattern` is applied.\n    \n        * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n                         resulting array's last entry will contain all input beyond the last\n                         matched pattern.\n        * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n                          array can be of any size.\n    \n        .. versionchanged:: 3.0\n           `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        array of separated strings.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n    >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n    [Row(s=['one', 'twoBthreeC'])]\n    >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n    [Row(s=['one', 'two', 'three', ''])]\n\n"
     ]
    }
   ],
   "source": [
    "help(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e212e72d-643b-4389-b63e-936a0700d5d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------+----------------+\n|id |email                      |subject         |\n+---+---------------------------+----------------+\n|1  |john.doe@example.com       |Math            |\n|1  |john.doe@example.com       |Science         |\n|1  |john.doe@example.com       |History         |\n|2  |jane.smith@example.com     |English         |\n|2  |jane.smith@example.com     |Art             |\n|2  |jane.smith@example.com     |Music           |\n|3  |alice.johnson@example.com  |Physics         |\n|3  |alice.johnson@example.com  |Chemistry       |\n|3  |alice.johnson@example.com  |Biology         |\n|4  |robert.brown@example.com   |Computer Science|\n|4  |robert.brown@example.com   |Programming     |\n|4  |robert.brown@example.com   |Data Structures |\n|5  |emily.lee@example.com      |Literature      |\n|5  |emily.lee@example.com      |Writing         |\n|5  |emily.lee@example.com      |Poetry          |\n|6  |michael.johnson@example.com|Physics         |\n|6  |michael.johnson@example.com|Chemistry       |\n|6  |michael.johnson@example.com|Biology         |\n|7  |sarah.davis@example.com    |Computer Science|\n|7  |sarah.davis@example.com    |Programming     |\n+---+---------------------------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "concated_courses = concat(\n",
    "    col(\"courses\")[0], lit(\",\"), col(\"courses\")[1], lit(\",\"), col(\"courses\")[2]\n",
    ").alias(\"courses_str\")\n",
    "\n",
    "df.select(\n",
    "    \"id\",\n",
    "    \"email\",\n",
    "    explode(split(concated_courses, \",\").alias(\"split_courses\")).alias(\"subject\"),\n",
    ").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "17_split",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
