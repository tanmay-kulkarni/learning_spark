{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00140f4-4721-40db-9417-9c2406b9c056",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+----------+------+-----------+\n| id| name|age|gender|department|salary|    address|\n+---+-----+---+------+----------+------+-----------+\n| 74|Emily| 21|  Male|      NULL| 51270|       NULL|\n| 68| NULL| 56|Female|        HR| 89595|       NULL|\n| 35| Mike| 44|  Male|        HR| 65925|    Chicago|\n| 74|Emily| 29|  NULL| Marketing| 96364|    Chicago|\n| 38|Emily| 31|Female|        HR| 76547|Los Angeles|\n| 70| NULL| 33|Female|      NULL| 97048|       NULL|\n| 74|Emily| 32|Female| Marketing| 56424|Los Angeles|\n| 72| John| 53|  Male|        HR| 60073|       NULL|\n| 76| John| 56|Female|      NULL| 67926|   New York|\n| 22| Mike| 23|  NULL|      NULL| 50860|Los Angeles|\n+---+-----+---+------+----------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define schema for employees dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), nullable=True),\n",
    "    StructField(\"name\", StringType(), nullable=True),\n",
    "    StructField(\"age\", IntegerType(), nullable=True),\n",
    "    StructField(\"gender\", StringType(), nullable=True),\n",
    "    StructField(\"department\", StringType(), nullable=True),\n",
    "    StructField(\"salary\", IntegerType(), nullable=True),\n",
    "    StructField(\"address\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Generate random data for employees dataframe\n",
    "data = []\n",
    "for _ in range(10):\n",
    "    id = random.randint(1, 100)\n",
    "    name = random.choice([\"John\", \"Jane\", \"Mike\", \"Emily\", None])\n",
    "    age = random.randint(20, 60)\n",
    "    gender = random.choice([\"Male\", \"Female\", None])\n",
    "    department = random.choice([\"Sales\", \"Marketing\", \"Finance\", \"HR\", None])\n",
    "    salary = random.randint(50000, 100000)\n",
    "    address = random.choice([\"New York\", \"Los Angeles\", \"Chicago\", None])\n",
    "    data.append((id, name, age, gender, department, salary, address))\n",
    "\n",
    "# Create employees dataframe\n",
    "employees = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show employees dataframe\n",
    "employees.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d29e65c-aa3e-44b7-b001-08a1e6857919",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### remove records where any of the values are null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341d5820-3e2f-4ad7-bc46-92bee420b1e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+----------+------+-----------+\n| id| name|age|gender|department|salary|    address|\n+---+-----+---+------+----------+------+-----------+\n| 35| Mike| 44|  Male|        HR| 65925|    Chicago|\n| 38|Emily| 31|Female|        HR| 76547|Los Angeles|\n| 74|Emily| 32|Female| Marketing| 56424|Los Angeles|\n+---+-----+---+------+----------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "employees.dropna().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7f4f69f-3725-4645-9ad3-8e254fa91fe8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### remove records where a subset of the columns contain null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a02f1b2c-bb46-4d27-a596-642d12f52e87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+----------+------+-----------+\n| id| name|age|gender|department|salary|    address|\n+---+-----+---+------+----------+------+-----------+\n| 68| NULL| 56|Female|        HR| 89595|       NULL|\n| 35| Mike| 44|  Male|        HR| 65925|    Chicago|\n| 74|Emily| 29|  NULL| Marketing| 96364|    Chicago|\n| 38|Emily| 31|Female|        HR| 76547|Los Angeles|\n| 74|Emily| 32|Female| Marketing| 56424|Los Angeles|\n| 72| John| 53|  Male|        HR| 60073|       NULL|\n+---+-----+---+------+----------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "employees.dropna(subset=['department']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9660dd98-7fae-44d0-9057-8903e797e83f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### define how the null records should be dropped - if all or any are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4daa0939-d75c-4bb3-919b-6e6829f9fc4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+----------+------+-----------+\n| id| name|age|gender|department|salary|    address|\n+---+-----+---+------+----------+------+-----------+\n| 74|Emily| 21|  Male|      NULL| 51270|       NULL|\n| 68| NULL| 56|Female|        HR| 89595|       NULL|\n| 35| Mike| 44|  Male|        HR| 65925|    Chicago|\n| 74|Emily| 29|  NULL| Marketing| 96364|    Chicago|\n| 38|Emily| 31|Female|        HR| 76547|Los Angeles|\n| 70| NULL| 33|Female|      NULL| 97048|       NULL|\n| 74|Emily| 32|Female| Marketing| 56424|Los Angeles|\n| 72| John| 53|  Male|        HR| 60073|       NULL|\n| 76| John| 56|Female|      NULL| 67926|   New York|\n+---+-----+---+------+----------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "employees.dropna(how='all', subset=['department', 'gender']).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "30_dropping_null_records",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
